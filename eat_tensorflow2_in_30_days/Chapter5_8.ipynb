{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0Fi06m+qHNkuV6m0pNxNk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anjasfedo/Learning-TensorFlow/blob/main/eat_tensorflow2_in_30_days/Chapter5_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-8 callbacks"
      ],
      "metadata": {
        "id": "ZJ8FVqtUTI4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The callbacks is `tf.keras` is a class, usually specified as a parameter when use `model.fit`. It provides the extra operations at the starting or the ending of training, each epoch or each batch. These operations include record some log information, change learning rate, early termination of the training, etc.\n",
        "\n",
        "Likewise, this callbacks parameter is also able to be specified for `model.evaluate` or `model.predict`, providing extra operations at the starting or the ending of the evaluation, prediction, or each batch. However this method is rarely used.\n",
        "\n",
        "For the most cases, the pre-defined callbacks in the sub-module `keras.callbacks` are sufficient. It is also possible to define child class inheriting `keras.callbacks.Callbacks` to costumize callbacks if necessary.\n",
        "\n",
        "All the classes of callbacks are inheriting `keras.callbacks.Callbacks`, which contain two attributes:\n",
        "- `params` is a dictionary, which records training parameters (e.g. verbosity, batch size, number of epochs, etc.).\n",
        "- `model` is the refference to the current model.\n",
        "\n",
        "Whats more, there is an extra argument `logs` in the certain methods of the callbacks classes, such as `on_epoch_begin`, `on_batch_end`. This parameter provides certain information of current epoch or batch and are able to save the computing results. These `logs` variables are able to transfer among the functions with the same name in these callbacks classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZnMi30o8TMVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Pre-defined Callbacks\n",
        "\n",
        "- `BaseLogger`: it calculates the mean metrics among all batches for each epoch. For those metrics with middle status in `stateful_metrics`, it uses the final metrics without calculating mean value of all the batches, and the final mean metrics is added to the variable `logs`. This callback is automatically applied to every Keras model and is applied first.\n",
        "- `History`: a dictionary that records the metrics of each epoch calculated by `BaseLogger` and is returned by `model.fit`. This callback is automatically applied to every Keras model after `BaseLogger`.\n",
        "- `EarlyStopping`: this callback terminates the training if the monitoring metrics are not significantly incraised after certain number of epoches.\n",
        "- `TensorBoard`: this callback saves the visualized log of the Tensorboard. It supports visualization of metrics, graphs, and parameters in the model.\n",
        "- `ModelChechpoint`: this callback saves model after each epoch.\n",
        "- `ReduceLROnPlateau` this callback reduce the learing rate with certain rate if the monitoring metrics are not significantly increased after certain number of epoches.\n",
        "- `TerminateOnNaN`: terminate the training if loss is NaN.\n",
        "- `LearingRateScheduler`: it controls the learning rate before each epoch with given function between the learning rate `lr` and epoch.\n",
        "- `CSVLogger`: save `logs` of each epoch in CSV file."
      ],
      "metadata": {
        "id": "mXBKUe0EVSOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Customized Callbacks\n",
        "\n",
        "It is possible to write a simple callback through `callback.LamdaCallback`, or write a complicated callback through inheriting base class `callbacks.Callback`."
      ],
      "metadata": {
        "id": "bWXUS_d8YAeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I7s7mTqWTISc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, losses, metrics, callbacks\n",
        "import tensorflow.keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of the simple callback using LambdaCallback\n",
        "\n",
        "import json\n",
        "\n",
        "json_log = open('log.json', mode='wt', buffering=1)\n",
        "json_logging_callback = callbacks.LambdaCallback(\n",
        "    on_epoch_end=lambda epoch, logs: json_log.write(\n",
        "        json.dumps(dict(epoch=epoch, **logs)) + '\\n'),\n",
        "    on_train_end=lambda logs: json_log.close()\n",
        ")"
      ],
      "metadata": {
        "id": "o2F94kQkYh-L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of complicated callback through base class inheritance. This is the source code of LearningRateScheduler.\n",
        "\n",
        "class LearningRateScheduler(callbacks.Callback):\n",
        "  def __init__(self, schedule, verbose=0):\n",
        "    super(LearningRateScheduler, self).__init__()\n",
        "    self.schedule = schedule\n",
        "    self.verbose = verbose\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    if not hasattr(self.model.optimizer, 'lr'):\n",
        "      raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
        "    try:\n",
        "      lr = float(K.get_value(self.model.optimizer.lr))\n",
        "      lr = self.schedule(epoch, lr)\n",
        "    except TypeError:\n",
        "      lr = self.schedule(epoch)\n",
        "\n",
        "    if not isinstance(lr, (tf.Tensor, float, np.float32, np.float64)):\n",
        "      raise ValueError('The output of the \"schedule\" function should be float.')\n",
        "\n",
        "    if isinstance(lr, ops.Tensor) and not lr.dtype.is_floating:\n",
        "      raise ValueError('The dtype of Tensor should be float')\n",
        "\n",
        "    K.set_value(self.model.optimizer.lr, K.get_value(lr))\n",
        "\n",
        "    if self.verbose > 0:\n",
        "      print('\\nEpoch %05d: LearningRateScheduler setting learning '\n",
        "            'rate to %s.' % (epoch + 1, lr))\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    logs = logs or {}\n",
        "    logs['lr'] = K.get_value(self.model.optimizer.lr)"
      ],
      "metadata": {
        "id": "DgtO-2w6Yh5f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBodmKlzYh1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDQ6jnTzYhwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MimF4NYIYetw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQ9VTRpPYe25"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}