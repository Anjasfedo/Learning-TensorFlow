{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd8sFWeWJLzqmD8YZi6x05",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anjasfedo/Learning-TensorFlow/blob/main/eat_tensorflow2_in_30_days/Chapter5_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-7 optimizers"
      ],
      "metadata": {
        "id": "sydMlPCwMx_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a group of magic cooks in machine learning. Their daily life looks like:\n",
        "\n",
        "- They grab some raw material (data)\n",
        "- Put them into a pot (model)\n",
        "- Light some file (optimization algorithm)\n",
        "- And wait until the cuisine is ready\n",
        "\n",
        "However, anyone who has cooking experience knows that file controlling is the key part. Even using same material with the same recipe, different fire level leads to totally diferent results: medium well, brunt, or still raw.\n",
        "\n",
        "This theory on cooking also applies to the machine learning. The choice of the optimization algorithm determines the final performance of the final model. An unsatisfying performance is not necessarily due to the problem of feature or model designing, instead, it might be attributed to the choice of optimization algorithm.\n",
        "\n",
        "The evolution of the optimization algoritm for the deep learning is: SGD -> SGDM -> NAG -> Adagrad -> Adadelta (RMSprop) -> Adam -> Nadam\n",
        "\n",
        "For the beginners, choosing Adam as the optimizaer and using default parameter will set everyting for you.\n",
        "\n",
        "Someresearchers who are chaising better metrics for publications could use Adam as the intial optimizer and use SGD later for fine-tuning the parameters for better performance.\n",
        "\n",
        "These are some cutting-edge optimization algorithm cleaiming a better performance, e.g. LazyAdam, Look-ahead, RAdam, Ranger, etc."
      ],
      "metadata": {
        "id": "AOP0V0qgM0oV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. How to Use the Optimizer\n",
        "\n",
        "Optimizer accepts variables and corresponding gradient through `apply_gradients` method to iterate over the given variables. Another way is using `minimize` method to optimize the target function iteratively.\n",
        "\n",
        "Another common way is passing the optimizer into `Model` of keras, and call `model.fit` method to optimize the loss function.\n",
        "\n",
        "A variable named `optimizer.iterations` will be created during optimizer initialization to record the number of iteration. Thus the optimizer should be created outside the decorator `@tf.function` with the same reason as `tf.Variable`."
      ],
      "metadata": {
        "id": "E_iPloPnOe8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "hSbjKMP4PXVF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Time stamp\n",
        "@tf.function\n",
        "def printbar():\n",
        "  ts = tf.timestamp()\n",
        "  today_ts = ts%(24*60*60)\n",
        "\n",
        "  hour = tf.cast(today_ts//3600+8, tf.int32)%tf.constant(24)\n",
        "  minute = tf.cast((today_ts%3600)//60, tf.int32)\n",
        "  second = tf.cast(tf.floor(today_ts%60), tf.int32)\n",
        "\n",
        "  def timeformat(m):\n",
        "    if tf.strings.length(tf.strings.format(\"{}\", m)) == 1:\n",
        "      return(tf.strings.format(\"0{}\",m))\n",
        "    else:\n",
        "      return(tf.strings.format(\"{}\",m))\n",
        "\n",
        "  timesting = tf.strings.join([timeformat(hour),timeformat(minute),\n",
        "                                timeformat(second)],separator = \":\")\n",
        "  tf.print(\"==========\"*8, end=\"\")\n",
        "  tf.print(timesting)"
      ],
      "metadata": {
        "id": "rFyewZS7PXQn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The minimal value of f(x) = a*x**2 + b*x + c\n",
        "# Here usage of optimizer.apply_gradient\n",
        "\n",
        "x = tf.Variable(0.0, name=\"x\", dtype=tf.float32)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "@tf.function\n",
        "def minimizef():\n",
        "  a = tf.constant(1.0)\n",
        "  b = tf.constant(-2.0)\n",
        "  c = tf.constant(1.0)\n",
        "\n",
        "  while tf.constant(True):\n",
        "    with tf.GradientTape() as tape:\n",
        "      y = a*tf.pow(x,2) + b*x + c\n",
        "    dy_dx = tape.gradient(y,x)\n",
        "    optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\n",
        "\n",
        "    # Condition of termaniting the iteration\n",
        "    if tf.abs(dy_dx) < tf.constant(0.00001):\n",
        "      break\n",
        "\n",
        "    if tf.math.mod(optimizer.iterations, 100) == 0:\n",
        "      printbar()\n",
        "      tf.print(\"step = \", optimizer.iterations)\n",
        "      tf.print(\"x = \", x)\n",
        "      tf.print(\"\")\n",
        "\n",
        "  y = a*tf.pow(x,2) + b*x + c\n",
        "  return y\n",
        "\n",
        "tf.print(\"y =\", minimizef())\n",
        "tf.print(\"x =\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbdd6owxPXLu",
        "outputId": "c06f283d-ad36-4dc0-bfd6-33c99e0a4014"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================11:23:14\n",
            "step =  100\n",
            "x =  0.867380381\n",
            "\n",
            "================================================================================11:23:14\n",
            "step =  200\n",
            "x =  0.98241204\n",
            "\n",
            "================================================================================11:23:14\n",
            "step =  300\n",
            "x =  0.997667611\n",
            "\n",
            "================================================================================11:23:14\n",
            "step =  400\n",
            "x =  0.999690711\n",
            "\n",
            "================================================================================11:23:14\n",
            "step =  500\n",
            "x =  0.999959\n",
            "\n",
            "================================================================================11:23:14\n",
            "step =  600\n",
            "x =  0.999994516\n",
            "\n",
            "y = 0\n",
            "x = 0.999995232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal value of f(x) = a*x**2 + b*x + c\n",
        "# Here usage of optimizer.minimize\n",
        "\n",
        "x = tf.Variable(0.0, name='x', dtype=tf.float32)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "def f():\n",
        "  a = tf.constant(1.0)\n",
        "  b = tf.constant(-2.0)\n",
        "  c = tf.constant(1.0)\n",
        "  y = a*tf.pow(x,2) + b*x + c\n",
        "  return y\n",
        "\n",
        "@tf.function\n",
        "def train(epoch=1000):\n",
        "  for _ in tf.range(epoch):\n",
        "    optimizer.minimize(f, [x])\n",
        "  tf.print(\"epoch = \", optimizer.iterations)\n",
        "  return (f())\n",
        "\n",
        "train(1000)\n",
        "tf.print(\"y = \", f())\n",
        "tf.print(\"x = \", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFiDJ6LcPXG6",
        "outputId": "451a65d7-b7bd-4098-d386-4e7b566cd23c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch =  1000\n",
            "y =  0\n",
            "x =  0.999998569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal value of f(x) = a*x**2 + b*x + c\n",
        "# Here is usage of model.fit\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "class FakeModel(tf.keras.models.Model):\n",
        "  def __init__(self, a, b, c):\n",
        "    super(FakeModel, self).__init__()\n",
        "    self.a = a\n",
        "    self.b = b\n",
        "    self.c = c\n",
        "\n",
        "  def build(self):\n",
        "    self.x = tf.Variable(0.0, name='x')\n",
        "    self.built = True\n",
        "\n",
        "  def call(self, input):\n",
        "    loss = self.a*(self.x)**2 + self.b*(self.x) + self.c\n",
        "    return (tf.ones_like(input) * loss)\n",
        "\n",
        "def myloss(y_true, y_pred):\n",
        "  return tf.reduce_mean(y_pred)\n",
        "\n",
        "model = FakeModel(tf.constant(1.0), tf.constant(-2.0), tf.constant(1.0))\n",
        "\n",
        "model.build()\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.01), loss=myloss)\n",
        "history = model.fit(tf.zeros((100,2)),\n",
        "                    tf.ones(100),\n",
        "                    batch_size=1,\n",
        "                    epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTlwr6xWPXCG",
        "outputId": "ed9c8162-83c6-43eb-8a01-e769f4b3c95e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"fake_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            "=================================================================\n",
            "Total params: 1 (4.00 Byte)\n",
            "Trainable params: 1 (4.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 1s 2ms/step - loss: 0.2481\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.0044\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 7.6740e-05\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 1.3500e-06\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 1.8477e-08\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.print(\"x=\",model.x)\n",
        "tf.print(\"loss=\",model(tf.constant(0.0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Auy9lIbwPW9O",
        "outputId": "ae4516f5-bf4d-4001-8c39-27838fe23daf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x= 0.999998569\n",
            "loss= 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Pre-defined Optimizers\n",
        "\n",
        "There are corresponding classes in `keras.optimizers` sub-module aas the implementation of optimizer.\n",
        "- `SGD`, the default parameters is for a pure SGD. For a non-zero parameter `momentum`, the optimizer changes to SGDM since it considers the first-order momentum. For `nestrov`=True, the optimizer changes to NAG (Nesterov Accelerated Gradient), which calculates the gradient of the one further step.\n",
        "- `Adagrad`, considers the second-order momentum and equiped with self-adaptive learning rate; the drawback is a slow learning rate at a later stage to early ceasing of learing due to the monotonocally descending learning rate.\n",
        "- `RMSprop`, considers the second-order momentum and equiped with self-adaptive learning rate; improves the `Adagrad` through exponential smooting, which only considers the second-order momentum in a given window length.\n",
        "- `Adadelta`, considers the second-order momentum, similiar as `RMSprop` but more complicated with an improvement self-adaptation.\n",
        "- `Adam`, consider both the first-order and the second-order momentum; it improves `RMSprop` by including first-order momentum.\n",
        "- `Nadam`, improves `Adam` by including Nesterov Acceleration.\n"
      ],
      "metadata": {
        "id": "sCas4egtiHiR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSzH57JNPW4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pBH-2X0Mveo"
      },
      "outputs": [],
      "source": []
    }
  ]
}