{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ7JaZ8T7jMqvS2qM7dJEh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anjasfedo/Learning-TensorFlow/blob/main/eat_tensorflow2_in_30_days/Chapter1_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-3 Example: Modeling Procedure for Texts"
      ],
      "metadata": {
        "id": "FcL2PFUbSJkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preparation\n",
        "\n",
        "The purpose of imdb dataset is to predict setiment label according to movie reviews.\n",
        "\n",
        "There 20000 text reviews in train dataset and 5000 in test datase, half positive and negative, respectively.\n",
        "\n",
        "The pre-processing of text dataset kinda complex, which include word devision (for chinese only, not relevant on this demo), dictionary construction, encoding, sequence filling, and data pipeline construction, etc.\n",
        "\n",
        "There is two popular method of text preparation in TensorFlow:\n",
        "1. construct text data generator using Tokenizer in `tf.keras.preprocessing`, together with `tf.kears.utils.Sequence`.\n",
        "2. with `tf.data.Dataset`, together with pre-processing layer `tf.keras.experimental.preprocessing.TextVectorization`\n",
        "\n",
        "Here is the second method"
      ],
      "metadata": {
        "id": "XE8WLy6aSLew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, preprocessing, optimizers, losses, metrics\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import re, string, os"
      ],
      "metadata": {
        "id": "MwXfrR5cSJQg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://raw.githubusercontent.com/lyhue1991/eat_tensorflow2_in_30_days/master/data/imdb/\"\n",
        "\n",
        "train_filename = \"train.csv\"\n",
        "test_filename = \"test.csv\"\n",
        "\n",
        "train_url = base_url + train_filename\n",
        "test_url = base_url + test_filename\n",
        "\n",
        "train_data_path = tf.keras.utils.get_file(train_filename, origin=train_url, cache_dir='.', cache_subdir='data')\n",
        "test_data_path = tf.keras.utils.get_file(test_filename, origin=test_url, cache_dir='.', cache_subdir='data')\n",
        "\n",
        "print(f\"Train data downloaded to: {train_data_path}\")\n",
        "print(f\"Test data downloaded to: {test_data_path}\")\n",
        "\n",
        "print(f\"Train data exists: {os.path.exists(train_data_path)}, Size: {os.path.getsize(train_data_path) / 1024:.2f} KB\")\n",
        "print(f\"Test data exists: {os.path.exists(test_data_path)}, Size: {os.path.getsize(test_data_path) / 1024:.2f} KB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYtro3ucSJK4",
        "outputId": "ca1a9806-943b-461c-c562-a844c72c6e68"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/lyhue1991/eat_tensorflow2_in_30_days/master/data/imdb/train.csv\n",
            "26683623/26683623 [==============================] - 0s 0us/step\n",
            "Downloading data from https://raw.githubusercontent.com/lyhue1991/eat_tensorflow2_in_30_days/master/data/imdb/test.csv\n",
            "6638231/6638231 [==============================] - 0s 0us/step\n",
            "Train data downloaded to: ./data/train.csv\n",
            "Test data downloaded to: ./data/test.csv\n",
            "Train data exists: True, Size: 26058.23 KB\n",
            "Test data exists: True, Size: 6482.65 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_WORDS = 10000 # consider the 10000 words with highest frequency of appearence\n",
        "MAX_LEN = 200 # each sample, preserve the first 200 words\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "FAuNXoWRT65u"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct data pipeline\n",
        "def split_line(line):\n",
        "  arr = tf.strings.split(line, sep='\\t')\n",
        "  label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[0]), tf.int32), axis=0)\n",
        "  text = tf.expand_dims(arr[1], axis=0)\n",
        "  return (text, label)"
      ],
      "metadata": {
        "id": "4RPUp0ppT606"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train_raw = tf.data.TextLineDataset(filenames=[train_data_path]) \\\n",
        "                .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
        "                .shuffle(buffer_size=10000) \\\n",
        "                .batch(BATCH_SIZE) \\\n",
        "                .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "ds_test_raw = tf.data.TextLineDataset(filenames=[test_data_path]) \\\n",
        "                .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
        "                .batch(BATCH_SIZE) \\\n",
        "                .prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "lmxN5kHjVXzi"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct dictionary\n",
        "def clean_text(text):\n",
        "  lowercase = tf.strings.lower(text)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  cleaned_punctuation = tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation), '')\n",
        "  return cleaned_punctuation"
      ],
      "metadata": {
        "id": "CgIB4NZBVHHw"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    standardize=clean_text,\n",
        "    split='whitespace',\n",
        "    max_tokens=MAX_WORDS,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_len\n",
        ")\n",
        "\n",
        "ds_text = ds_train_raw.map(lambda text, label: text)\n",
        "vectorize_layer.adapt(ds_text)\n",
        "print(vectorize_layer.get_vocabulary()[0:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60a2dlj9T6wP",
        "outputId": "ae59eadd-8402-47ad-f863-406c32de47cc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'on', 'not', 'you', 'his', 'are', 'have', 'be', 'he', 'one', 'its', 'at', 'all', 'by', 'an', 'they', 'from', 'who', 'so', 'like', 'her', 'just', 'or', 'about', 'has', 'if', 'out', 'some', 'there', 'what', 'good', 'more', 'when', 'very', 'she', 'even', 'my', 'no', 'would', 'up', 'time', 'only', 'which', 'story', 'really', 'their', 'were', 'had', 'see', 'can', 'me', 'than', 'we', 'much', 'well', 'get', 'been', 'will', 'into', 'people', 'also', 'other', 'do', 'bad', 'because', 'great', 'first', 'how', 'him', 'most', 'dont', 'made', 'then', 'them', 'films', 'movies', 'way', 'make', 'could', 'too', 'any']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word encoding\n",
        "ds_train = ds_train_raw.map(lambda text, label: (vectorize_layer(text), label)) \\\n",
        "            .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "ds_test = ds_test_raw.map(lambda text, label: (vectorize_layer(text), label)) \\\n",
        "            .prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "yQir-4JKT6rQ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model Definition\n",
        "\n",
        "Here is the way to customized modeling by inherit base class `Model`"
      ],
      "metadata": {
        "id": "Wzm6WBwoX9yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Actually, modeling with sequential() or API functions should be priorized.\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "class CnnModel(models.Model):\n",
        "  def __init__(self):\n",
        "    super(CnnModel, self).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.embedding = layers.Embedding(MAX_WORDS, 7, input_length=MAX_LEN)\n",
        "    self.conv_1 = layers.Conv1D(16, kernel_size=5, name='conv_1', activation='relu')\n",
        "    self.pool_1 = layers.MaxPool1D(name='pool_1')\n",
        "    self.conv_2 = layers.Conv1D(128, kernel_size=2, name='conv_2', activation='relu')\n",
        "    self.pool_2 = layers.MaxPool1D(name='pool_2')\n",
        "    self.flatten = layers.Flatten()\n",
        "    self.dense = layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "    x = self.conv_1(x)\n",
        "    x = self.pool_1(x)\n",
        "    x = self.conv_2(x)\n",
        "    x = self.pool_2(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense(x)\n",
        "    return (x)\n",
        "\n",
        "  def summary(self):\n",
        "    x_input = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
        "    output = self.call(x_input)\n",
        "    model = models.Model(inputs=x_input, outputs=output)\n",
        "    model.summary()\n",
        "\n",
        "model = CnnModel()\n",
        "model.build(input_shape=(None, MAX_LEN))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQvuAQrcT6m_",
        "outputId": "2c6c16df-5f8f-44c1-c61a-94881bd25c9a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 200, 7)            70000     \n",
            "                                                                 \n",
            " conv_1 (Conv1D)             (None, 196, 16)           576       \n",
            "                                                                 \n",
            " pool_1 (MaxPooling1D)       (None, 98, 16)            0         \n",
            "                                                                 \n",
            " conv_2 (Conv1D)             (None, 97, 128)           4224      \n",
            "                                                                 \n",
            " pool_2 (MaxPooling1D)       (None, 48, 128)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6144)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 6145      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80945 (316.19 KB)\n",
            "Trainable params: 80945 (316.19 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rmxU-NyGT6h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o24MuGTaSJFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J2Ir4mftSI_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b8DQbZBhSI54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Crt_kMO8SI0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wdoBVy7dSIvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HVSvxRc0SIqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RjUw2bzwSIly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EH9ZZsPLSIf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1KHtDtBxSIbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hwtGRv5LSIV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktCYi_hbSFwM"
      },
      "outputs": [],
      "source": []
    }
  ]
}