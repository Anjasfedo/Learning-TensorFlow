{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnA0Blwig7kuJngbG1jN96",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anjasfedo/Learning-TensorFlow/blob/main/eat_tensorflow2_in_30_days/Chapter1_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-3 Example: Modeling Procedure for Texts"
      ],
      "metadata": {
        "id": "FcL2PFUbSJkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preparation\n",
        "\n",
        "The purpose of imdb dataset is to predict setiment label according to movie reviews.\n",
        "\n",
        "There 20000 text reviews in train dataset and 5000 in test datase, half positive and negative, respectively.\n",
        "\n",
        "The pre-processing of text dataset kinda complex, which include word devision (for chinese only, not relevant on this demo), dictionary construction, encoding, sequence filling, and data pipeline construction, etc.\n",
        "\n",
        "There is two popular method of text preparation in TensorFlow:\n",
        "1. construct text data generator using Tokenizer in `tf.keras.preprocessing`, together with `tf.kears.utils.Sequence`.\n",
        "2. with `tf.data.Dataset`, together with pre-processing layer `tf.keras.experimental.preprocessing.TextVectorization`\n",
        "\n",
        "Here is the second method"
      ],
      "metadata": {
        "id": "XE8WLy6aSLew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, preprocessing, optimizers, losses, metrics\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import re, string, os"
      ],
      "metadata": {
        "id": "MwXfrR5cSJQg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://raw.githubusercontent.com/lyhue1991/eat_tensorflow2_in_30_days/master/data/imdb/\"\n",
        "\n",
        "train_filename = \"train.csv\"\n",
        "test_filename = \"test.csv\"\n",
        "\n",
        "train_url = base_url + train_filename\n",
        "test_url = base_url + test_filename\n",
        "\n",
        "train_data_path = tf.keras.utils.get_file(train_filename, origin=train_url, cache_dir='.', cache_subdir='data')\n",
        "test_data_path = tf.keras.utils.get_file(test_filename, origin=test_url, cache_dir='.', cache_subdir='data')\n",
        "\n",
        "print(f\"Train data downloaded to: {train_data_path}\")\n",
        "print(f\"Test data downloaded to: {test_data_path}\")\n",
        "\n",
        "print(f\"Train data exists: {os.path.exists(train_data_path)}, Size: {os.path.getsize(train_data_path) / 1024:.2f} KB\")\n",
        "print(f\"Test data exists: {os.path.exists(test_data_path)}, Size: {os.path.getsize(test_data_path) / 1024:.2f} KB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYtro3ucSJK4",
        "outputId": "ca1a9806-943b-461c-c562-a844c72c6e68"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/lyhue1991/eat_tensorflow2_in_30_days/master/data/imdb/train.csv\n",
            "26683623/26683623 [==============================] - 0s 0us/step\n",
            "Downloading data from https://raw.githubusercontent.com/lyhue1991/eat_tensorflow2_in_30_days/master/data/imdb/test.csv\n",
            "6638231/6638231 [==============================] - 0s 0us/step\n",
            "Train data downloaded to: ./data/train.csv\n",
            "Test data downloaded to: ./data/test.csv\n",
            "Train data exists: True, Size: 26058.23 KB\n",
            "Test data exists: True, Size: 6482.65 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_WORDS = 10000 # consider the 10000 words with highest frequency of appearence\n",
        "MAX_LEN = 200 # each sample, preserve the first 200 words\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "FAuNXoWRT65u"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct data pipeline\n",
        "def split_line(line):\n",
        "  arr = tf.strings.split(line, sep='\\t')\n",
        "  label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[0]), tf.int32), axis=0)\n",
        "  text = tf.expand_dims(arr[1], axis=0)\n",
        "  return (text, label)"
      ],
      "metadata": {
        "id": "4RPUp0ppT606"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train_raw = tf.data.TextLineDataset(filenames=[train_data_path]) \\\n",
        "                .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
        "                .shuffle(buffer_size=10000) \\\n",
        "                .batch(BATCH_SIZE) \\\n",
        "                .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "ds_test_raw = tf.data.TextLineDataset(filenames=[test_data_path]) \\\n",
        "                .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
        "                .batch(BATCH_SIZE) \\\n",
        "                .prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "lmxN5kHjVXzi"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct dictionary\n",
        "def clean_text(text):\n",
        "  lowercase = tf.strings.lower(text)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  cleaned_punctuation = tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation), '')\n",
        "  return cleaned_punctuation"
      ],
      "metadata": {
        "id": "CgIB4NZBVHHw"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    standardize=clean_text,\n",
        "    split='whitespace',\n",
        "    max_tokens=MAX_WORDS,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_len\n",
        ")\n",
        "\n",
        "ds_text = ds_train_raw.map(lambda text, label: text)\n",
        "vectorize_layer.adapt(ds_text)\n",
        "print(vectorize_layer.get_vocabulary()[0:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60a2dlj9T6wP",
        "outputId": "ae59eadd-8402-47ad-f863-406c32de47cc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'on', 'not', 'you', 'his', 'are', 'have', 'be', 'he', 'one', 'its', 'at', 'all', 'by', 'an', 'they', 'from', 'who', 'so', 'like', 'her', 'just', 'or', 'about', 'has', 'if', 'out', 'some', 'there', 'what', 'good', 'more', 'when', 'very', 'she', 'even', 'my', 'no', 'would', 'up', 'time', 'only', 'which', 'story', 'really', 'their', 'were', 'had', 'see', 'can', 'me', 'than', 'we', 'much', 'well', 'get', 'been', 'will', 'into', 'people', 'also', 'other', 'do', 'bad', 'because', 'great', 'first', 'how', 'him', 'most', 'dont', 'made', 'then', 'them', 'films', 'movies', 'way', 'make', 'could', 'too', 'any']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word encoding\n",
        "ds_train = ds_train_raw.map(lambda text, label: (vectorize_layer(text), label)) \\\n",
        "            .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "ds_test = ds_test_raw.map(lambda text, label: (vectorize_layer(text), label)) \\\n",
        "            .prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "yQir-4JKT6rQ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model Definition\n",
        "\n",
        "Here is the way to customized modeling by inherit base class `Model`"
      ],
      "metadata": {
        "id": "Wzm6WBwoX9yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Actually, modeling with sequential() or API functions should be priorized.\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "class CnnModel(models.Model):\n",
        "  def __init__(self):\n",
        "    super(CnnModel, self).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.embedding = layers.Embedding(MAX_WORDS, 7, input_length=MAX_LEN)\n",
        "    self.conv_1 = layers.Conv1D(16, kernel_size=5, name='conv_1', activation='relu')\n",
        "    self.pool_1 = layers.MaxPool1D(name='pool_1')\n",
        "    self.conv_2 = layers.Conv1D(128, kernel_size=2, name='conv_2', activation='relu')\n",
        "    self.pool_2 = layers.MaxPool1D(name='pool_2')\n",
        "    self.flatten = layers.Flatten()\n",
        "    self.dense = layers.Dense(1, activation='sigmoid')\n",
        "    super(CnnModel,self).build(input_shape)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs)\n",
        "    x = self.conv_1(x)\n",
        "    x = self.pool_1(x)\n",
        "    x = self.conv_2(x)\n",
        "    x = self.pool_2(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense(x)\n",
        "    return (x)\n",
        "\n",
        "  def summary(self):\n",
        "    x_input = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
        "    output = self.call(x_input)\n",
        "    model = models.Model(inputs=x_input, outputs=output)\n",
        "    model.summary()\n",
        "\n",
        "model = CnnModel()\n",
        "model.build(input_shape=(None, MAX_LEN))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQvuAQrcT6m_",
        "outputId": "84b66180-08c8-4360-b6df-fa1412b40249"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 200, 7)            70000     \n",
            "                                                                 \n",
            " conv_1 (Conv1D)             (None, 196, 16)           576       \n",
            "                                                                 \n",
            " pool_1 (MaxPooling1D)       (None, 98, 16)            0         \n",
            "                                                                 \n",
            " conv_2 (Conv1D)             (None, 97, 128)           4224      \n",
            "                                                                 \n",
            " pool_2 (MaxPooling1D)       (None, 48, 128)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6144)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 6145      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80945 (316.19 KB)\n",
            "Trainable params: 80945 (316.19 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Training\n",
        "\n",
        "Here the customized training loop method"
      ],
      "metadata": {
        "id": "tO5psM5CZkis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Time stamp\n",
        "@tf.function\n",
        "def printbar():\n",
        "  ts = tf.timestamp()\n",
        "  today_ts = tf.timestamp()%(24*60*60)\n",
        "\n",
        "  hour = tf.cast(tf.floor(today_ts/3600), tf.int32)\n",
        "  minute = tf.cast(tf.floor((today_ts%3600)/60), tf.int32)\n",
        "  second = tf.cast(tf.floor(today_ts%60), tf.int32)\n",
        "\n",
        "  def timeformat(m):\n",
        "    if tf.strings.length(tf.strings.format('{}', m)) == 1:\n",
        "      return tf.strings.format('0{}', m)\n",
        "    else:\n",
        "      return tf.strings.format('{}', m)\n",
        "\n",
        "  timestring = tf.strings.join([timeformat(hour), timeformat(minute), timeformat(second)], separator=':')\n",
        "\n",
        "  tf.print('========'*8 + '\\n' + timestring)"
      ],
      "metadata": {
        "id": "rmxU-NyGT6h3"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optimizers.Nadam()\n",
        "loss_fn = losses.BinaryCrossentropy()"
      ],
      "metadata": {
        "id": "BSO6stg0aZft"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = metrics.Mean(name='train_loss')\n",
        "train_metric = metrics.BinaryAccuracy(name='train_accruracy')\n",
        "\n",
        "valid_loss = metrics.Mean(name='valid_loss')\n",
        "valid_metric = metrics.BinaryAccuracy(name='valid_accruracy')"
      ],
      "metadata": {
        "id": "o24MuGTaSJFC"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(model, features, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(features, training=True)\n",
        "    loss = loss_fn(labels, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss.update_state(loss)\n",
        "  train_metric.update_state(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def valid_step(model, features, labels):\n",
        "  predictions = model(features, training=False)\n",
        "  batch_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  valid_loss.update_state(batch_loss)\n",
        "  valid_metric.update_state(labels, predictions)"
      ],
      "metadata": {
        "id": "J2Ir4mftSI_o"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, ds_train, ds_valid, epochs):\n",
        "  for epoch in tf.range(1, epochs + 1):\n",
        "\n",
        "    for features, labels in ds_train:\n",
        "      train_step(model, features, labels)\n",
        "\n",
        "    for features, labels in ds_valid:\n",
        "      valid_step(model, features, labels)\n",
        "\n",
        "    logs = 'Epoch={}, Loss={}, Accuracy={}, Valid Loss={}, Valid Accuracy={}'\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "                print(\"=\" * 50)\n",
        "                print(f\"Epoch {epoch}/{epochs}, \"\n",
        "                        f\"Loss: {train_loss.result():.4f}, \"\n",
        "                        f\"Accuracy: {train_metric.result():.4f}, \"\n",
        "                        f\"Valid Loss: {valid_loss.result():.4f}, \"\n",
        "                        f\"Valid Accuracy: {valid_metric.result():.4f}\")\n",
        "                print(\"=\" * 50)\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_metric.reset_states()\n",
        "    valid_loss.reset_states()\n",
        "    valid_metric.reset_states()\n",
        "\n",
        "train_model(model, ds_train, ds_test, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GizvJ-LIdMKR",
        "outputId": "54bbc7b1-20d8-4ac9-c342-581844f75b1b"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Epoch 1/10, Loss: 0.4567, Accuracy: 0.7563, Valid Loss: 0.3198, Valid Accuracy: 0.8678\n",
            "==================================================\n",
            "==================================================\n",
            "Epoch 2/10, Loss: 0.2336, Accuracy: 0.9073, Valid Loss: 0.3339, Valid Accuracy: 0.8710\n",
            "==================================================\n",
            "==================================================\n",
            "Epoch 3/10, Loss: 0.1561, Accuracy: 0.9406, Valid Loss: 0.3916, Valid Accuracy: 0.8648\n",
            "==================================================\n",
            "==================================================\n",
            "Epoch 4/10, Loss: 0.0936, Accuracy: 0.9677, Valid Loss: 0.5181, Valid Accuracy: 0.8554\n",
            "==================================================\n",
            "==================================================\n",
            "Epoch 5/10, Loss: 0.0452, Accuracy: 0.9850, Valid Loss: 0.6891, Valid Accuracy: 0.8530\n",
            "==================================================\n",
            "==================================================\n",
            "Epoch 6/10, Loss: 0.0199, Accuracy: 0.9944, Valid Loss: 0.9018, Valid Accuracy: 0.8494\n",
            "==================================================\n",
            "==================================================\n",
            "Epoch 7/10, Loss: 0.0111, Accuracy: 0.9967, Valid Loss: 1.0939, Valid Accuracy: 0.8498\n",
            "==================================================\n",
            "==================================================\n",
            "Epoch 8/10, Loss: 0.0102, Accuracy: 0.9966, Valid Loss: 1.2206, Valid Accuracy: 0.8508\n",
            "==================================================\n",
            "==================================================\n",
            "Epoch 9/10, Loss: 0.0131, Accuracy: 0.9954, Valid Loss: 1.3358, Valid Accuracy: 0.8442\n",
            "==================================================\n",
            "==================================================\n",
            "Epoch 10/10, Loss: 0.0216, Accuracy: 0.9921, Valid Loss: 1.3089, Valid Accuracy: 0.8480\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Evaluation\n",
        "\n",
        "The trained model by costomized looping is not compiler, so method `model.evaluate()` cant be applied directly"
      ],
      "metadata": {
        "id": "0WnmpTRVd_aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model,ds_valid):\n",
        "    for features, labels in ds_valid:\n",
        "         valid_step(model,features,labels)\n",
        "    # logs = 'Valid Loss={},Valid Accuracy={}'\n",
        "    # tf.print(tf.strings.format(logs,(valid_loss.result(),valid_metric.result())))\n",
        "    print(f'Valid Loss: {valid_loss.result():.4f}, Valid Accuracy: {valid_metric.result():.4f}')\n",
        "\n",
        "    valid_loss.reset_states()\n",
        "    train_metric.reset_states()\n",
        "    valid_metric.reset_states()"
      ],
      "metadata": {
        "id": "b8DQbZBhSI54"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, ds_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNA-UROKg2Xb",
        "outputId": "119c7f12-8797-412f-8052-fca78c417518"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Loss: 1.3089, Valid Accuracy: 0.8480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Application\n",
        "\n",
        "There are some available methods:\n",
        "- model.predict()\n",
        "- model()\n",
        "- model.call()\n",
        "- model.predict_on_batch()\n",
        "\n",
        "recomend to use `model.predict()` method, since it can be applied on both Dataset and Tensor"
      ],
      "metadata": {
        "id": "d2Bo9iHFiTIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(ds_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdoBVy7dSIvo",
        "outputId": "d143be7d-9f0d-4227-a351-f4ceada8f9c7"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 2s 8ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.99999976],\n",
              "       [0.9998863 ],\n",
              "       [0.99853206],\n",
              "       ...,\n",
              "       [0.9999656 ],\n",
              "       [0.68822837],\n",
              "       [1.        ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x_test,_ in ds_test.take(1):\n",
        "    print(model(x_test))\n",
        "    #Indentical expressions:\n",
        "    #print(model.call(x_test))\n",
        "    #print(model.predict_on_batch(x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVSvxRc0SIqx",
        "outputId": "f7291197-41dd-434f-e315-753b5d031ad3"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[9.9999976e-01]\n",
            " [9.9988627e-01]\n",
            " [9.9853206e-01]\n",
            " [1.6907185e-16]\n",
            " [5.0385017e-04]\n",
            " [5.3917755e-09]\n",
            " [1.0308210e-07]\n",
            " [7.3073439e-05]\n",
            " [9.9999332e-01]\n",
            " [7.2912651e-01]\n",
            " [9.8259407e-01]\n",
            " [9.9999899e-01]\n",
            " [3.6092793e-08]\n",
            " [9.9998492e-01]\n",
            " [1.1585158e-07]\n",
            " [8.3854403e-03]\n",
            " [2.8352773e-10]\n",
            " [8.2858754e-03]\n",
            " [5.1688805e-04]\n",
            " [8.9920485e-01]\n",
            " [1.8289582e-11]\n",
            " [1.0000000e+00]\n",
            " [9.9985754e-01]\n",
            " [8.1316625e-09]\n",
            " [1.0000000e+00]\n",
            " [9.9999434e-01]\n",
            " [9.8755842e-01]\n",
            " [4.6968147e-01]\n",
            " [9.9993324e-01]\n",
            " [9.9864715e-01]\n",
            " [1.6044406e-04]\n",
            " [9.9997693e-01]], shape=(32, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Saving\n",
        "\n",
        "The originial way to save model in TensorFlow that recommended"
      ],
      "metadata": {
        "id": "om1gYHqEiwql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/model/tf_model_savedmodel', save_format='tf')\n",
        "print('export saved model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjUw2bzwSIly",
        "outputId": "0291982c-fe6b-4d32-a334-f5f422da5fee"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "export saved model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_loaded = tf.keras.models.load_model('/content/model/tf_model_savedmodel')\n",
        "model_loaded.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH9ZZsPLSIf4",
        "outputId": "856ffd71-da9b-469f-ba5e-7af391436f23"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cnn_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  70000     \n",
            "                                                                 \n",
            " conv_1 (Conv1D)             multiple                  576       \n",
            "                                                                 \n",
            " pool_1 (MaxPooling1D)       multiple                  0         \n",
            "                                                                 \n",
            " conv_2 (Conv1D)             multiple                  4224      \n",
            "                                                                 \n",
            " pool_2 (MaxPooling1D)       multiple                  0         \n",
            "                                                                 \n",
            " flatten (Flatten)           multiple                  0         \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  6145      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80945 (316.19 KB)\n",
            "Trainable params: 80945 (316.19 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_loaded.predict(ds_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwtGRv5LSIV6",
        "outputId": "e22e2720-42aa-4bf9-edf0-3a4531b5b23f"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 2s 9ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.99999976],\n",
              "       [0.9998863 ],\n",
              "       [0.99853206],\n",
              "       ...,\n",
              "       [0.9999656 ],\n",
              "       [0.68822837],\n",
              "       [1.        ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktCYi_hbSFwM"
      },
      "outputs": [],
      "source": []
    }
  ]
}