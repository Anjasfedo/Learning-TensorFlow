{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKAS6ESBb4Dh0u1v8kiM81",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anjasfedo/Learning-TensorFlow/blob/main/eat_tensorflow2_in_30_days/Chapter5_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-5 losses"
      ],
      "metadata": {
        "id": "UZIlz9J9gn6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, the target function in supervised learning consists of loss function and regularization term. (Target = Loss = Regularization)\n",
        "\n",
        "For the keras model, the regularization term of the target function is usually designated in each layer, such as using `kernel_regularizer` and `bias_regularizer` parameters in `Dense` layer to specify using l1 or l2 norm. On other hand, `kernel_constrain` and `bias_constrain` parameters can limit the range of the weights, which is also a method of regularization.\n",
        "\n",
        "Loss function is designated during the compilation of the model. For the regression models, the most popular loss function is the mean squared error `mean_squared_error`.\n",
        "\n",
        "For binary classification model, the most popular loss function is binary cross entropy `binary_crossentropy`.\n",
        "\n",
        "For multiple classification model, when the label ar one-hot encoded, we shoud use categorical cross entropy `categorical_crossentropy` as loss function; for the category with ordinal encoding, the sparse categorical cross entropy `sparse_categorical_crossentropy` should be used as the loss function.\n",
        "\n",
        "We may define customized loss function when necessary. The customized loss function requires two tensor `y_true` and `y_pred` as input, and it output a scalar as the value of the calculated loss function."
      ],
      "metadata": {
        "id": "gd-bvbOPgo_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, losses, regularizers, constraints"
      ],
      "metadata": {
        "id": "_G9Yhr_3iQGD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function and Regularization Term"
      ],
      "metadata": {
        "id": "oA0HhdUSiZNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64,\n",
        "                       input_dim=64,\n",
        "                       kernel_regularizer=regularizers.l2(0.01),\n",
        "                       activity_regularizer=regularizers.l1(0.01),\n",
        "                       kernel_constraint=constraints.MaxNorm(max_value=2, axis=0)))\n",
        "model.add(layers.Dense(10,\n",
        "                       kernel_regularizer=regularizers.l1_l2(0.01, 0.01),\n",
        "                       activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['AUC'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqengnd9iQCV",
        "outputId": "068039c7-08bb-4366-a0c8-f279f1e25e6d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4810 (18.79 KB)\n",
            "Trainable params: 4810 (18.79 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Pre-defined Loss Function\n",
        "\n",
        "There are two types of implementation of the pre-defined loss function:\n",
        "- class-type\n",
        "- function-tyoe\n",
        "\n",
        "e.g. `CategoricalCrossentropy` and `categorical_crossentropy` are both categorical cross entropy; the former is the implementation by class, and the latter is the by function.\n",
        "\n",
        "The most frequently used pre-defined function are:\n",
        "- mean_square_error (Mean Squared Error, for regression, dubbed as \"mse\", class-type and function-type are implementations are `MeanSquaredError` and `MSE`, respectively)\n",
        "- mean_absolute_error (Mean Absolute Error, for regression, dubbed as \"mae\", class-type and function-type implementations are `MeanAbsoluteError` and `MAE`, respectively)\n",
        "- mean_absolute_percentage_error (Mean Absolute Percentage Error, for regression dubbed as \"mape\", class-type and function-type implementations are `MeanAbsolutePercentageError` and `MAPE`, respectively)\n",
        "- Huber (Huber Loss, for regression, performance between \"mse\" and \"mae\", robust to outliers, thus has advantages comparint to \"mse\"; implemented only in class)\n",
        "- binary_crossentropy (Binary Cross Entropy, for binary classification; the class-type implementation is `BinaryCrossentropy`)\n",
        "- categorical_crossentropy (Categorical Cross Entropy, for multiple classification, requires one-hot encoding for the label; the class-type implementation is `CategoricalCrossentropy`)\n",
        "- sparse_categorical_crossentropy (Sparse Cross Entropy, used for multiple classification, requires ordinal encoding; the class-type implementation is `SparseCategoricalCrossentropy`)\n",
        "- hinge (Hinge Loss Function, for binary classification, famous for the application as loss function in Support Vector Machine (SVM); the class-type implementation is `Hinge`)\n",
        "- kld (Kullback-Leibler Divergence Loss, usually used as the loss function in the expectation maximization (EM) algoritm; it is a measurement of the difference between two probability distributions. The class-type and function-type implementations are `KLDivergence` and `KLD`, respectively)\n",
        "- cosine_similarity (Cosine similarity, for multiple classification; the class-type implementation is `CosineSimilarity`)"
      ],
      "metadata": {
        "id": "eltsJ8qajd6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Customized Loss Function\n",
        "\n",
        "The customized loss function requires two tensors `y_true` and `y_pred` as input, and it output a scalar as the value of the calculated loss function.\n",
        "\n",
        "It is also possible to customize loss function through inheriting from base class `tf.keras.losses.Loss` and rewrite the `call` method to implement the calculation of loss.\n",
        "\n",
        "Here is an example of customized implementation to the Focal Loss, which is an improvement of `binary_crossentropy` loss function.\n",
        "\n",
        "Focal loss results better conmparing to the binary corss entropy, given condition of unbalanced category and many easy samples in training data.\n",
        "\n",
        "It has two adjustable parameters, aplha and gamma. The aim of alpha is to decay the weight of negative samples, and gamma to decay the weight of easy sampels.\n",
        "\n",
        "So the model will then focal its weight on the positive samples and hard samples. This is why the function is called focal loss."
      ],
      "metadata": {
        "id": "Z2B2L2E4meXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def focal_loss(gamma=2., aplha=0.75):\n",
        "  def focal_loss_fixed(y_true, y_pred):\n",
        "    bce = tf.losses.binary_crossentropy(y_true, y_pred)\n",
        "    p_t = (y_true * y_pred) + ((1 - y_true) * (1 - y_pred))\n",
        "    alpha_factor = y_true * aplha + (1 - y_true) * (1 - aplha)\n",
        "    modulating_factor = tf.pow((1.0 - p_t), gamma)\n",
        "    loss = tf.reduce_sum(alpha_fector * modulation_factor * bce, axis=-1)\n",
        "    return loss\n",
        "  return focal_loss_fixed"
      ],
      "metadata": {
        "id": "YB1YTuOGiP-Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(losses.Loss):\n",
        "  def __init__(self, gamma=2., alpha=0.75, name='focal_loss'):\n",
        "    self.gamma = gamma\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def call(self, y_true, y_pred):\n",
        "    bce = tf.losses.binary_crossentropy(y_true, y_pred)\n",
        "    p_t = (y_true * y_pred) + ((1 - y_true) * (1 - y_pred))\n",
        "    alpha_factor = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)\n",
        "    modulating_factor = tf.pow((1.0 - p_t), self.gamma)\n",
        "    loss = tf.reduce_sum(alpha_fector * modulation_factor * bce, axis=-1)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "fQjRnniSiP6d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_jhOr48aiP2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ftU_0KfoiPyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lY14HFVYiPtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "psRYGM2eiPo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5qZD746TiOpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMuMSVyMgji1"
      },
      "outputs": [],
      "source": []
    }
  ]
}