{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmpAVi+sXxELjBmUG7Bt5U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anjasfedo/Learning-TensorFlow/blob/main/eat_tensorflow2_in_30_days/Chapter5_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-3 activation"
      ],
      "metadata": {
        "id": "Ae-l9Ata_4qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation function plays a key role in deep learning. It introduces the non-linearity that enables the neural network to fit arbitary complicated functions.\n",
        "\n",
        "The neural network, no matter how complicated the structure is, is still a linear transformation which cannot fit the non-linear functions without the activation function.\n",
        "\n",
        "For the time being, the most popular activation is `relu`, but there is some new functiosn such as `swish`, `GELU`, claiming a better performance over `relu`"
      ],
      "metadata": {
        "id": "iFUMnzJQBJbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The most popular activation functions\n",
        "\n",
        "- `tf.nn.sigmoid`: Compressing real number between 0 to 1, usually used in the output layer for binary classification; the main drawback are vanishing gradient, high computing complexity, and the non-zero center of the output.\n",
        "\n",
        "- `tf.nn.softmax`: Extended version of sigmoid for multiple categories, usually used in the output layer for multiple classifications.\n",
        "\n",
        "- `tf.nn.tanh`: Compressing real number between -1 to 1, expectation of the output is zero; the main drawbacks are vanishing gradient and high computing complexity.\n",
        "\n",
        "- `tf.nn.relu`: Linear reactified unit, the most popular activation function, usually used in the hidden layer; the main drawbacks are non-zero center of the output and vanishing gradient for the inputs < 0 (dying relu)\n",
        "\n",
        "- `tf.nn.leaky_relu`: Improved ReLU, resolving the dying ReLU problem.\n",
        "\n",
        "- `tf.nn.elu`: Exponential linear unit, which is an improvement to the ReLU, alleviate the dying ReLU problem.\n",
        "\n",
        "- `tf.nn.selu`: Scaled exponential linear unit, which is able to normalize the neural network automatically if the weights are initalized through `tf.keras.initalizers.lecun_normal`. No gradient exploding/vanishing problems, but need to apply together with AlphaDropout (an alternation of Dropout)\n",
        "\n",
        "- `tf.nn.swish`: Self-gated activation function, a research product from Google. The literature prove that it brings slight improvement comparing to ReLU.\n",
        "\n",
        "- `gelu`: Gaussian error linear unit, which has the best performance in Transformer; however `tf.nn` hasnt implemented it."
      ],
      "metadata": {
        "id": "11_fEJAbC4d9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Implementing activation functions in the models\n",
        "\n",
        "There are two ways of implementing activation functions in Keras models:\n",
        "- specifying through are `activation` parameter in certain layers.\n",
        "- adding activation layer `layers.Activation` explicitly."
      ],
      "metadata": {
        "id": "LRoPY4h_FtQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "AkVzu0eiG4BR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsACPd3X-Rq1",
        "outputId": "2eea9d9a-f955-4ad5-a03c-cd759fd04226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, None, 32)          544       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, None, 10)          330       \n",
            "                                                                 \n",
            " activation (Activation)     (None, None, 10)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 874 (3.41 KB)\n",
            "Trainable params: 874 (3.41 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(32, activation=tf.nn.relu, input_shape=(None, 16))) # specifying through the activation parameter\n",
        "\n",
        "model.add(layers.Dense(10))\n",
        "model.add(layers.Activation(tf.nn.softmax)) # adding activation layer explicitly\n",
        "\n",
        "model.summary()"
      ]
    }
  ]
}