{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgiokfbQf5yFibqS3REWc2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anjasfedo/Learning-TensorFlow/blob/main/eat_tensorflow2_in_30_days/Chapter5_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-4 layers"
      ],
      "metadata": {
        "id": "mQpy9By_1SZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The deep learning model usually consist of various layers.\n",
        "\n",
        "`tf.keras.layers` contains a large amount of models with various functions, such as:\n",
        "\n",
        "- `layers.Dense`\n",
        "- `layers.Flatten`\n",
        "- `layers.Input`\n",
        "- `layers.DenseFeature`\n",
        "- `layers.Dropout`\n",
        "- `layers.Conv2D`\n",
        "- `layers.MaxPooling2D`\n",
        "- `layers.Conv1D`\n",
        "- `layers.Embedding`\n",
        "- `layers.GRU`\n",
        "- `layers.LSTM`\n",
        "- `layers.Bidirectional`\n",
        "- etc.\n",
        "\n",
        "In case these pre-defined layers are insufficient for modeling, the users are able to write anonymous layer `tf.keras.Lambda` or write customized layer through inheriting `tf.keras.layers.Layer`.\n",
        "\n",
        "Note that `tf.keras.Lambda` is only for the layers without any trainable parameter."
      ],
      "metadata": {
        "id": "jrzL8XPd1aFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Pre-defined Layer\n",
        "\n",
        "Here are the introduction for the most popular layers.\n",
        "\n",
        "***Fundamental Layers***\n",
        "- Dense: Densely connected layer. # of parameters = # of features of the input layer x # of weight + # of bias.\n",
        "- Activation: Activation function layer. Usually placed after the Dense layer for specific the activation function in the Dense layer.\n",
        "- Dropout: Dropout layer. Setting the inputs as zero randomly during the training stage, which is a method of regularization.\n",
        "- BatchNormalization: Layer for batch normalization. It scale and translate the bathces into stable mean and standart deviation through linear transformation. This lead to the model adaptive to the various distribution of the input, which is mild regularization that accelerates training. This layer is usually applied before the activation function.\n",
        "- SpatialDropout2D: Spatial dropout layer. Setting the whole feature map into zero with certain possibilities during training, which is a regularization to avoid high correlation between feature maps.\n",
        "- Input: Input layer. Usually it is the first layer when modeling through functional API.\n",
        "- DenseFeature: Layer that provides connection to the feature column, which is used to accept a list of columns and generate a dense connected layer.\n",
        "- Flatten: Flatten layer, used for flattening multi-dimensional tensor into on-dimension.\n",
        "- Reshape: Reshape layer, reform the shape of the input tensor.\n",
        "- Concatenate: Concatenating layer to link multiple tensors along the specific dimension.\n",
        "- Add: Adding layer.\n",
        "- Subtract: Subtracting layer.\n",
        "- Maximum: Layer for maximum value.\n",
        "- Minimun: Layer for minimum value.\n",
        "\n",
        "***Layers for the Convolutional network.***\n",
        "- `Conv1D`: Layer of 1D convolutional, usually for text. # of parameters = # of input channels x # of kernel size (e.g. 3) x # of kernels.\n",
        "- `Conv2D`: Layer of 2D convolutional, usually for image. # of parameters = # of input channels x # of kernel size (e.g. 3x3) x # of kernels.\n",
        "- `Conv3D`: Layer of 3D convolutional, usually for video. # of parameters = # of input channels x # of kernel size (e.g. 3x3x3) x # of kernels.\n",
        "- `SaparableConv2D`: Depthwise 2D separable convolution. Unlike the traditional convolutional, separable convolutions consist in first performing a depthwise spatial convolutional (which acts on each input channel saparately) followed by a pointwise convolutional which mixes together the resulting output channels. # of parameters = # of input channels x size of convolutional kernel + # of channels x 1 x 1 x # of output channels. Usually, depthwise separable convolution has a much smaller number of parameters with a better performance.\n",
        "- `DepthwiseConv2D`: Depthwise 2D convolution. Depthwise convolutions consists in performing just the first step in a depthwise separable convolution (which acts on each input channels separately). Usually the # of input and output channels are the same, but can be altered through the `depth_multiplier` argument to control how many output channels are generated per input channel in the depthwise. # of output channels = # of input channels x depth_multiplier. # of input channels x size of kernel x `depth_multiplier`.\n",
        "- `Conv2DTranspose`: 2D Transposed convolution layer (sometimes called Deconvolution), buth this is on the inverse of operation of convolution. When the kernel maintains the same as convolution, and given the input size the same as the output size of convolution, then the output size of the transposed convolution is the same as the input size of convolution.\n",
        "- `LocallyConnected2D`: Locally-connected layer for 2D inputs. This layer works similiary to the `Conv2D` layer, expect that weights are unshared, thus has much more parameter than `Conv2D`.\n",
        "- `MaxPooling2D`: 2D max pooling layer, also called down-sampling layer. This is for reducing dimension without any trainable parameter.\n",
        "- `AverangePooling2D`: 2D averange pooling layer.\n",
        "- `GlobalMaxPool2D`: Global max pooling layer. Only one value preserved for each channel, usually used between convolution layer and fully connected layer, which is subtitution of `Flatten`.\n",
        "- `GlobalAVgPool2D`: Gloval average pooling layer. Only one value preserved for each channel.\n",
        "\n",
        "***Recursive network related layers***\n",
        "- `Embedding`: Embedding layer, provides an encoding with higher efficiency than one-hot for discrite features. It is usually used for projecting input words into dense vectors. Training is required for the parameters in the embedding layer.\n",
        "- `LSTM`: Long Short-Term Memory Layer, which is the most popular layer for the recursive network. It contains carry track and is composed of a cell, an input gate, an output gate, and a forget gate, which significantly alleviate the problem of gradient vanishing and thus applicable for the problem of long-term dependency. All the middle results could be observed when the parameter `return_sequences` is set as a `True`; in the opposite case only the final result is returned.\n",
        "- `GRU`: Gated Recursive Unit Layer, a simplified version of LSTM without carry track, thus has less parameters and could be trained faster.\n",
        "- `SimpleRNN`: Simple Recursive Neural Network Layer. It is not popular due to the problem of gradient vanishing, which made it inapplicable to the long-dependence.\n",
        "- `ConvLSTM2D`: Convolutional LSTM Layer. Has similiar structure to LSTM but the conversion operation to both input and status are convolution.\n",
        "- `Bidirectional`: Bi-directional wrapper for RNNs, for wrapping layers (such as LSTM and GRU) into bi-directional RNN, enhancing the capability of feature extracting.\n",
        "- `RNN`: Base classes of RNN. It accepts an RNN cell or a list of RNN cells. and iterate on the sequence to convert the input as an RNN through the calling of `tf.keras.backend.rnn` function.\n",
        "- `LSTMCell`: LSTM Cell. Unlike iterating across the whole sequence as `LSTM`, it only iterate once the sequence. It would be more intuitive to understand the LSTM equals to `LSTMCell` wrapped by the base layer `RNN`.\n",
        "- `GRUCell`: GRU Cell. Unlike iterating across the whole sequences as `GRU`, it only iterate once on the sequence.\n",
        "- `SimpeleRNNCell`: SimpleRNN Cell. Unlike iterating across the whole sequence as `SimpleRNN`, it only iterate once on the sequence.\n",
        "- `AbstractRNNCell`: Abstract RNN Cell. It allows user to customize RNN through inheritance and subsequently construct the RNN layer through wrapping these RNN cells by RNN base layer.\n",
        "- `Attention`: Dot-product attention layer, a.k.a. Luong-style attention, for constructing attention model.\n",
        "- `AdditiveAttention`: Additive attention layer, a.k.a. Bahdanau-style attention, for constructing attention model.\n",
        "- `TimeDistributed`: Time distributed wrapper, allows applying `Dense`, `Conv2D` to each time segment."
      ],
      "metadata": {
        "id": "IjX8axAW20Xn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Customized Model Layer\n",
        "\n",
        "It is recommended to use `Lambda` layer for customized model layer without trainable parameter.\n",
        "\n",
        "For the customized model layer with trainable parameters, it is recommended to inherit from the base class `Layer`.\n",
        "\n",
        "We only need to define forward propagation for `Lambda` layer since there is no trainable parameter, thus it is easier in application comparing to the inheritance from base class `Layer`.\n",
        "\n",
        "The forward propagation of `Lambda` layer could be expressed using `lambda ` function or keyword `def` in Python."
      ],
      "metadata": {
        "id": "kG_tuKZWExJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "\n",
        "my_power = layers.Lambda(lambda x: tf.math.pow(x, 2))\n",
        "my_power(tf.range(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-khI7a3R-cv4",
        "outputId": "39392db5-d4bc-43a3-d8bd-2910fb295050"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 0,  1,  4,  9, 16], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inheriting from `Layer` needs re-implementation of the initialization, `build` and `call` methods. Here is an example of simplified linear layer, which is similiar to `Dense`."
      ],
      "metadata": {
        "id": "NS7Y8_tlFcDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(layers.Layer):\n",
        "  def __init__(self, units=32, **kwargs):\n",
        "    super(Linear, self).__init__(**kwargs)\n",
        "    self.units = units\n",
        "\n",
        "  # The trainable parameters are defined in build method\n",
        "  def build(self, input_shape):\n",
        "    self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                             initializer='random_normal',\n",
        "                             trainable=True) # Parameter named \"w\" is compulsory or an error will be throuwn out\n",
        "    self.b = self.add_weight(shape=(self.units,),\n",
        "                             initializer='random_normal',\n",
        "                             trainable=True)\n",
        "    super(Linear, self).build(input_shape) # Identical to self.built = True\n",
        "\n",
        "  # The logic for forward propagation is defined in call method, and is called by __call__ method\n",
        "  @tf.function\n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "  # Use customized get-config method to save the model as h5 format, specifically for the model composed through Functional API with customized Layer\n",
        "  def get_config(self):\n",
        "    config = super(Linear, self).get_config()\n",
        "    config.update({'units': self.units})\n",
        "    return config"
      ],
      "metadata": {
        "id": "OkAkwRv-Fr1J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear = Linear(units=0)\n",
        "print(linear.built)\n",
        "\n",
        "# Specific input_shape, call build method; the 0th dimension is for the number of samples, should be filled with None.\n",
        "linear.build(input_shape=(None, 16))\n",
        "print(linear.built)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuwEHmDhFrtp",
        "outputId": "eb6207ef-6ecc-4805-f950-e2766b6cf70c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear = Linear(units=8)\n",
        "print(linear.built)\n",
        "\n",
        "linear.build(input_shape=(None, 16))\n",
        "print(linear.compute_output_shape(input_shape=(None, 16)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idkKTMzXFrpC",
        "outputId": "5d6c55c1-3fa6-46bc-c1b9-2b391bcb143f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "(None, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear = Linear(units=16)\n",
        "print(linear.built)\n",
        "\n",
        "# If built = False, the __call__ method will call \"build\" first, then call \"call\" method\n",
        "linear(tf.random.uniform((100, 64)))\n",
        "print(linear.built)\n",
        "\n",
        "config = linear.get_config()\n",
        "print(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdgq6Uc6HL8a",
        "outputId": "bceabfc0-7721-4857-abc6-9ff768e95188"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n",
            "{'name': 'linear_3', 'trainable': True, 'dtype': 'float32', 'units': 16}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = models.Sequential()\n",
        "# Note: the input_shape here will be modified by the model, so we don't have to fill None in the dimension representing the number of samples.\n",
        "model.add(Linear(units=1, input_shape=(2,)))\n",
        "print(f\"model.input_shape: {model.input_shape}\")\n",
        "print(f\"model.output_shape: {model.output_shape}\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b2GsLyFHL4z",
        "outputId": "8a54eba6-d3c5-4471-9759-7cb1315e6166"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.input_shape: (None, 2)\n",
            "model.output_shape: (None, 1)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " linear (Linear)             (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3 (12.00 Byte)\n",
            "Trainable params: 3 (12.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='sgd', loss='mse', metrics=['mae'])\n",
        "print(model.predict(tf.constant([[3.0, 2.0], [4.0, 5.0]])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0QCBlBRHL1N",
        "outputId": "66307888-524b-49f5-b984-f4a8cd0b686c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 113ms/step\n",
            "[[-0.05707848]\n",
            " [-0.05766379]]\n"
          ]
        }
      ]
    }
  ]
}